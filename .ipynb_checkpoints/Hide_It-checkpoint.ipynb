{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simplejson in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (3.19.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install simplejson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Scep9wt2Q_Rn"
   },
   "outputs": [],
   "source": [
    "# CONNECT TO REDDIT THROUGH UAUTH\n",
    "\n",
    "import requests\n",
    "\n",
    "# developer account info\n",
    "client_id = \"iBUlXklHFr1KDzhJ7BVdAw\"\n",
    "secret = \"9xDTwhK5AkhCKm27i0uM9UcQcH_DjQ\"\n",
    "uname = \"hide-it-AI4G\"\n",
    "pw = \"AI4GoodLab\"\n",
    "\n",
    "  # nrequest OAuth token\n",
    "auth = requests.auth.HTTPBasicAuth(client_id, secret)\n",
    "\n",
    "# login method (username & pw)\n",
    "data = {'grant_type': 'password', 'username': uname, 'password': pw}\n",
    "\n",
    "# setup header info (brief description of the app from the github)\n",
    "headers = {'User-Agent': 'Hide-It: https://github.com/Rain1618/Hide-It (use for training a model in the AI4Good Lab, Montreal cohort)'}\n",
    "\n",
    "# send request for an OAuth token\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token', auth=auth, data=data, headers=headers)\n",
    "\n",
    "# convert response to JSON and pull access_token value\n",
    "TOKEN = res.json()['access_token']\n",
    "\n",
    "# add authorization to headers dictionary\n",
    "headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n",
    "\n",
    "# while the token is valid (~2 hours) add headers=headers to requests\n",
    "request = requests.get('https://oauth.reddit.com/api/v1/me', headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x__3RkJ33gj0",
    "outputId": "4dc8123c-d62c-455b-f30f-8dfc55d91d86"
   },
   "outputs": [],
   "source": [
    "# DOWNLOAD DATA FROM API TO CSV\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# target subreddits\n",
    "subreddits = ['EatingDisorder',\n",
    "              'DomesticViolence', \n",
    "              'Addiction', \n",
    "              'SuicideWatch', \n",
    "              'SexualAssault', \n",
    "              'explainlikeimfive',\n",
    "              'casualconversation'\n",
    "            ]\n",
    "target_info = ['title', 'selftext', 'subreddit', 'name']\n",
    "  \n",
    "for sub in subreddits:\n",
    "    \n",
    "    data = pd.DataFrame(columns = ['title', 'selftext', 'subreddit', 'name'])\n",
    "    count = 0\n",
    "    # change limit to 100\n",
    "    limit = 100\n",
    "    search_after = None\n",
    "    \n",
    "    # will be 43,000 (300,000 / 7 categories)\n",
    "    \n",
    "    while (count < 43000):\n",
    "        \n",
    "        url  = \"https://oauth.reddit.com/r/\" + sub + \"/top.json\"\n",
    "        res = requests.get(url, headers=headers, params={'limit': limit,'after': search_after})\n",
    "        count = count + limit\n",
    "        posts = res.json()['data']['children']\n",
    "        \n",
    "        for post in posts:\n",
    "\n",
    "            post_data = {key: post['data'][key] for key in target_info}\n",
    "            data = data.append(post_data, ignore_index=True)\n",
    "\n",
    "        # if close to going over request limit, sleep for time remaining until reset\n",
    "\n",
    "        if float(res.headers['X-Ratelimit-Remaining']) < 5:\n",
    "            time.sleep(float(res.headers['X-Ratelimit-Reset']))\n",
    "\n",
    "        # save the last post as the index to search before, otherwise will get duplicates on iterations\n",
    "        search_after = data['name'].iloc[len(data) - 1]\n",
    "        \n",
    "    # save data from this subreddit\n",
    "    data.to_csv(sub + \"_data.csv\", encoding='utf-8')\n",
    "    \n",
    "        \n",
    "#data.to_csv('reddit_data.csv', encoding='utf-8')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOAD DATA FROM API TO CSV\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# target subreddits\n",
    "subreddits = ['EatingDisorder']\n",
    "target_info = ['title', 'selftext', 'subreddit', 'name']\n",
    "  \n",
    "for sub in subreddits:\n",
    "    \n",
    "    data = pd.DataFrame(columns = ['title', 'selftext', 'subreddit', 'name'])\n",
    "    count = 0\n",
    "    # change limit to 100\n",
    "    limit = 10\n",
    "    enough_data = 100\n",
    "    search_after = None\n",
    "    \n",
    "    print(len(data))\n",
    "    \n",
    "    # will be 43,000 (300,000 / 7 categories)\n",
    "    \n",
    "    while len(data) < enough_data:\n",
    "        \n",
    "        url = \"https://oauth.reddit.com/r/\" + sub + \"/new.json\"\n",
    "        res = requests.get(url, headers=headers, params={'limit': limit,'after': search_after})\n",
    "        posts = res.json()['data']['children']\n",
    "        print(type(posts))\n",
    "        print(\"num posts: \", len(posts))\n",
    "        \n",
    "        for post in posts:\n",
    "\n",
    "            post_data = {key: post['data'][key] for key in target_info}\n",
    "            data = data.append(post_data, ignore_index=True)\n",
    "\n",
    "        # if close to going over request limit, sleep for time remaining until reset\n",
    "\n",
    "        if float(res.headers['X-Ratelimit-Remaining']) < 5:\n",
    "            time.sleep(float(res.headers['X-Ratelimit-Reset']))\n",
    "            print(\"sleeping zzzzz\")\n",
    "\n",
    "        # save the last post as the index to search before, otherwise will get duplicates on iterations\n",
    "        search_after = data['name'].iloc[len(data) - 1]\n",
    "\n",
    "        print(\"length of dataframe: \", len(data))\n",
    "        print(data.tail(1))\n",
    "        \n",
    "    # save data from this subreddit\n",
    "    data.to_csv(sub + \"_data.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
